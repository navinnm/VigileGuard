name: VigileGuard CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ published ]
  schedule:
    # Run security scan daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: read
  security-events: write
  actions: read
  issues: write  # For creating issues on security findings
  pull-requests: write  # For PR comments

env:
  PYTHON_VERSION: '3.8'

jobs:
  # Code Quality Checks
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install black flake8 mypy bandit safety

      - name: Check code formatting with Black
        run: |
          black --check --diff vigileguard.py
          echo "âœ… Code formatting check passed"

      - name: Lint with flake8
        run: |
          flake8 vigileguard.py --max-line-length=100 --ignore=E203,W503 --statistics
          echo "âœ… Linting check passed"

      - name: Type checking with mypy
        run: |
          mypy vigileguard.py --ignore-missing-imports
          echo "âœ… Type checking passed"
        continue-on-error: true

      - name: Security scan with bandit
        run: |
          bandit -r . -f json -o bandit-report.json || true
          echo "âœ… Security scan completed"

      - name: Dependency security check
        run: |
          safety check --json --output safety-report.json || true
          echo "âœ… Dependency security check completed"

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  # Functional Testing (Enhanced)
  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, ubuntu-20.04]
        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test VigileGuard basic functionality
        run: |
          echo "ğŸ§ª Testing basic commands..."
          python vigileguard.py --help
          python vigileguard.py --version
          echo "âœ… Basic functionality tests passed"

      - name: Test JSON output
        run: |
          echo "ğŸ§ª Testing JSON output generation..."
          python vigileguard.py --format json --output test-report.json || true
          
          if [ -f test-report.json ]; then
            echo "âœ… JSON file created successfully"
            python -c "
          import json
          with open('test-report.json', 'r') as f:
              data = json.load(f)
              assert 'scan_info' in data, 'Missing scan_info'
              assert 'summary' in data, 'Missing summary'
              assert 'findings' in data, 'Missing findings'
              print('âœ… JSON structure is valid')
          "
            python -c "
          import json
          with open('test-report.json', 'r') as f:
              data = json.load(f)
              total = data['summary']['total_findings']
              print(f'ğŸ“Š Found {total} total findings')
              for severity, count in data['summary']['by_severity'].items():
                  print(f'  {severity}: {count}')
          "
            echo "âœ… JSON output test completed successfully"
          else
            echo "âŒ JSON file was not created"
            exit 1
          fi

      - name: Test with custom config
        run: |
          echo "ğŸ§ª Testing custom configuration..."
          python vigileguard.py --config config.yaml --format json --output config-test.json || true
          
          if [ -f config-test.json ]; then
            echo "âœ… Custom config test passed"
          else
            echo "âŒ Config test failed"
            exit 1
          fi

      - name: Test error handling
        run: |
          echo "ğŸ§ª Testing error handling..."
          echo "invalid: yaml: content" > invalid-config.yaml
          python vigileguard.py --config invalid-config.yaml --format json || true
          echo "âœ… Error handling test completed"

      - name: Performance benchmark
        run: |
          echo "âš¡ Running performance benchmark..."
          time python vigileguard.py --format json > /dev/null || true
          echo "âœ… Performance benchmark completed"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}-${{ matrix.os }}
          path: |
            test-report.json
            config-test.json
          retention-days: 30

  # Docker Testing (Enhanced)
  docker:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: [quality]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          docker build -t vigileguard:test .
          echo "âœ… Docker image built successfully"

      - name: Test Docker image functionality
        run: |
          echo "ğŸ³ Testing Docker image..."
          docker run --rm vigileguard:test --help
          docker run --rm vigileguard:test --version
          docker run --rm vigileguard:test --format json > docker-test-report.json || true
          
          if [ -f docker-test-report.json ] && [ -s docker-test-report.json ]; then
            echo "âœ… Docker JSON output test passed"
          else
            echo "âŒ Docker JSON output test failed"
            exit 1
          fi

      - name: Run Trivy on Docker image
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'vigileguard:test'
          format: 'sarif'
          output: 'docker-trivy-results.sarif'

      - name: Upload Docker security scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'docker-trivy-results.sarif'

      - name: Upload Docker test results
        uses: actions/upload-artifact@v4
        with:
          name: docker-test-results
          path: |
            docker-test-report.json
            docker-trivy-results.sarif
          retention-days: 30

  # Installation Script Testing
  install-test:
    name: Installation Script Test
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Test install script syntax
        run: |
          chmod +x install.sh
          bash -n install.sh
          echo "âœ… Install script syntax check passed"

      - name: Test install script help
        run: |
          ./install.sh --help
          ./install.sh --version
          echo "âœ… Install script help test passed"

      - name: Test installation process
        run: |
          echo "ğŸ§ª Testing installation process..."
          # Create a test environment
          mkdir -p test-install
          cd test-install
          
          # Test git clone simulation
          git clone .. VigileGuard-test
          cd VigileGuard-test
          
          # Test dependency installation
          pip install -r requirements.txt
          
          # Test basic functionality after install
          python vigileguard.py --help
          echo "âœ… Installation process test passed"

  # Security Testing (Enhanced)
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: python

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

  # Cross-platform compatibility test
  compatibility:
    name: Cross-Platform Test
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, ubuntu-20.04, ubuntu-22.04]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test platform compatibility
        run: |
          echo "ğŸ§ª Testing on ${{ matrix.os }}..."
          python vigileguard.py --format json --output platform-test.json || true
          
          if [ -f platform-test.json ]; then
            echo "âœ… Platform compatibility test passed on ${{ matrix.os }}"
          else
            echo "âŒ Platform compatibility test failed on ${{ matrix.os }}"
            exit 1
          fi

  # VigileGuard Demo (Enhanced)
  demo:
    name: VigileGuard Security Demo
    runs-on: ubuntu-latest
    needs: [test]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run VigileGuard Demo
        run: |
          echo "ğŸ›¡ï¸ VigileGuard Security Audit Demo"
          echo "=================================="
          echo ""
          echo "Running security audit on GitHub Actions runner..."
          echo ""
          
          python vigileguard.py --format console || true
          
          echo ""
          echo "ğŸ“Š Generating detailed JSON report..."
          python vigileguard.py --format json --output demo-report.json || true
          
          if [ -f demo-report.json ]; then
            echo ""
            echo "ğŸ“‹ Security Audit Summary:"
            python -c "
          import json
          with open('demo-report.json', 'r') as f:
              data = json.load(f)
              print(f'ğŸ” Total findings: {data[\"summary\"][\"total_findings\"]}')
              for severity, count in data['summary']['by_severity'].items():
                  if count > 0:
                      emoji = 'ğŸ”´' if severity in ['CRITICAL', 'HIGH'] else 'ğŸŸ¡' if severity == 'MEDIUM' else 'ğŸ”µ'
                      print(f'{emoji} {severity}: {count}')
              print('')
              print('âœ… VigileGuard successfully identified security issues!')
              print('ğŸ“– This demonstrates the tool is working correctly.')
          "
          fi

      - name: Generate demo badge
        run: |
          echo "ğŸ·ï¸ Generating demo badge..."
          python -c "
          import json
          with open('demo-report.json', 'r') as f:
              data = json.load(f)
              total = data['summary']['total_findings']
              critical_high = data['summary']['by_severity'].get('CRITICAL', 0) + data['summary']['by_severity'].get('HIGH', 0)
              
              # Create a simple badge data
              badge_data = {
                  'schemaVersion': 1,
                  'label': 'VigileGuard Demo',
                  'message': f'{total} findings ({critical_high} critical/high)',
                  'color': 'red' if critical_high > 0 else 'yellow' if total > 0 else 'green'
              }
              
              with open('demo-badge.json', 'w') as badge_file:
                  json.dump(badge_data, badge_file, indent=2)
          "

      - name: Upload demo results
        uses: actions/upload-artifact@v4
        with:
          name: vigileguard-demo
          path: |
            demo-report.json
            demo-badge.json
          retention-days: 30

  # Documentation and Examples Testing
  docs:
    name: Documentation Test
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Verify README examples
        run: |
          echo "ğŸ“š Testing README examples..."
          
          # Test all example commands in README work
          python vigileguard.py --help > /dev/null
          python vigileguard.py --version > /dev/null
          python vigileguard.py --format json --output readme-test.json || true
          
          if [ -f readme-test.json ]; then
            echo "âœ… README examples work correctly"
          else
            echo "âŒ README examples failed"
            exit 1
          fi

      - name: Check documentation completeness
        run: |
          echo "ğŸ“‹ Checking documentation completeness..."
          
          # Verify all essential files exist
          test -f README.md && echo "âœ… README.md exists"
          test -f requirements.txt && echo "âœ… requirements.txt exists"
          test -f config.yaml && echo "âœ… config.yaml exists"
          test -f install.sh && echo "âœ… install.sh exists"
          test -f Dockerfile && echo "âœ… Dockerfile exists"
          test -f Makefile && echo "âœ… Makefile exists"
          
          # Check README has essential sections
          grep -q "Installation" README.md && echo "âœ… Installation section exists"
          grep -q "Usage" README.md && echo "âœ… Usage section exists"
          grep -q "CI/CD" README.md && echo "âœ… CI/CD section exists"
          grep -q "Contributing" README.md && echo "âœ… Contributing section exists"

  # Release preparation
  release-check:
    name: Release Readiness
    runs-on: ubuntu-latest
    if: github.event_name == 'release'
    needs: [quality, test, docker, install-test, security, compatibility]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify release readiness
        run: |
          echo "ğŸš€ Checking release readiness..."
          
          # Check version consistency
          VERSION=$(python vigileguard.py --version 2>/dev/null | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+' || echo "1.0.0")
          echo "Current version: $VERSION"
          
          # Verify all tests passed
          echo "âœ… All tests passed"
          echo "âœ… Security scans completed"
          echo "âœ… Cross-platform compatibility verified"
          echo "âœ… Release is ready for deployment"

      - name: Create release artifacts
        run: |
          echo "ğŸ“¦ Creating release artifacts..."
          
          # Create a release archive
          tar -czf vigileguard-${GITHUB_REF_NAME}.tar.gz \
            vigileguard.py requirements.txt config.yaml \
            install.sh Dockerfile Makefile README.md
          
          echo "âœ… Release artifacts created"

      - name: Upload release artifacts
        uses: actions/upload-artifact@v4
        with:
          name: release-artifacts-${{ github.ref_name }}
          path: vigileguard-${{ github.ref_name }}.tar.gz
          retention-days: 90

  # Notification and status reporting
  notify:
    name: Notification
    runs-on: ubuntu-latest
    needs: [quality, test, docker, install-test]
    if: always()
    
    steps:
      - name: Determine overall status
        id: status
        run: |
          if [[ "${{ needs.quality.result }}" == "success" && "${{ needs.test.result }}" == "success" && "${{ needs.docker.result }}" == "success" && "${{ needs.install-test.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=All VigileGuard tests passed! ğŸ‰" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Some VigileGuard tests failed. Check the logs." >> $GITHUB_OUTPUT
          fi

      - name: Create status comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const status = '${{ steps.status.outputs.status }}';
            const message = '${{ steps.status.outputs.message }}';
            const emoji = status === 'success' ? 'âœ…' : 'âŒ';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${emoji} **VigileGuard CI/CD Pipeline**\n\n${message}\n\n**Test Results:**\n- Code Quality: ${{ needs.quality.result }}\n- Functionality Tests: ${{ needs.test.result }}\n- Docker Build: ${{ needs.docker.result }}\n- Installation: ${{ needs.install-test.result }}`
            });

      - name: Final status check
        run: |
          echo "ğŸ›¡ï¸ VigileGuard CI/CD Pipeline Complete"
          echo "======================================"
          echo "Status: ${{ steps.status.outputs.status }}"
          echo "Message: ${{ steps.status.outputs.message }}"
          
          if [[ "${{ steps.status.outputs.status }}" == "failure" ]]; then
            exit 1
          fi