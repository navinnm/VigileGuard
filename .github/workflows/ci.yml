name: VigileGuard CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ published ]
  schedule:
    # Run security scan daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: read
  security-events: write
  actions: read
  issues: write  # For creating issues on security findings
  pull-requests: write  # For PR comments

env:
  PYTHON_VERSION: '3.8'

jobs:
  # Code Quality Checks
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install black flake8 mypy bandit safety

      - name: Check code formatting with Black
        run: |
          black --check --diff vigileguard.py
          echo "‚úÖ Code formatting check passed"

      - name: Lint with flake8
        run: |
          flake8 vigileguard.py --max-line-length=100 --ignore=E203,W503 --statistics
          echo "‚úÖ Linting check passed"

      - name: Type checking with mypy
        run: |
          mypy vigileguard.py --ignore-missing-imports
          echo "‚úÖ Type checking passed"
        continue-on-error: true

      - name: Security scan with bandit
        run: |
          bandit -r . -f json -o bandit-report.json || true
          echo "‚úÖ Security scan completed"

      - name: Dependency security check
        run: |
          safety check --json --output safety-report.json || true
          echo "‚úÖ Dependency security check completed"

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  # Functional Testing (Enhanced)
  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, ubuntu-20.04]
        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test VigileGuard basic functionality
        run: |
          echo "üß™ Testing basic commands..."
          python vigileguard.py --help
          python vigileguard.py --version
          echo "‚úÖ Basic functionality tests passed"

      - name: Test JSON output
        run: |
          echo "üß™ Testing JSON output generation..."
          python vigileguard.py --format json --output test-report.json || true
          
          if [ -f test-report.json ]; then
            echo "‚úÖ JSON file created successfully"
            python -c "
          import json
          with open('test-report.json', 'r') as f:
              data = json.load(f)
              assert 'scan_info' in data, 'Missing scan_info'
              assert 'summary' in data, 'Missing summary'
              assert 'findings' in data, 'Missing findings'
              print('‚úÖ JSON structure is valid')
          "
            python -c "
          import json
          with open('test-report.json', 'r') as f:
              data = json.load(f)
              total = data['summary']['total_findings']
              print(f'üìä Found {total} total findings')
              for severity, count in data['summary']['by_severity'].items():
                  print(f'  {severity}: {count}')
          "
            echo "‚úÖ JSON output test completed successfully"
          else
            echo "‚ùå JSON file was not created"
            exit 1
          fi

      - name: Test with custom config
        run: |
          echo "üß™ Testing custom configuration..."
          python vigileguard.py --config config.yaml --format json --output config-test.json || true
          
          if [ -f config-test.json ]; then
            echo "‚úÖ Custom config test passed"
          else
            echo "‚ùå Config test failed"
            exit 1
          fi

      - name: Test error handling
        run: |
          echo "üß™ Testing error handling..."
          echo "invalid: yaml: content" > invalid-config.yaml
          python vigileguard.py --config invalid-config.yaml --format json || true
          echo "‚úÖ Error handling test completed"

      - name: Performance benchmark
        run: |
          echo "‚ö° Running performance benchmark..."
          time python vigileguard.py --format json > /dev/null || true
          echo "‚úÖ Performance benchmark completed"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}-${{ matrix.os }}
          path: |
            test-report.json
            config-test.json
          retention-days: 30

  # Docker Testing (Enhanced)
  docker:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: [quality]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          docker build -t vigileguard:test .
          echo "‚úÖ Docker image built successfully"

      - name: Test Docker image functionality
        run: |
          echo "üê≥ Testing Docker image..."
          docker run --rm vigileguard:test --help
          docker run --rm vigileguard:test --version
          docker run --rm vigileguard:test --format json > docker-test-report.json || true
          
          if [ -f docker-test-report.json ] && [ -s docker-test-report.json ]; then
            echo "‚úÖ Docker JSON output test passed"
          else
            echo "‚ùå Docker JSON output test failed"
            exit 1
          fi

      - name: Run Trivy on Docker image
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'vigileguard:test'
          format: 'sarif'
          output: 'docker-trivy-results.sarif'

      - name: Upload Docker security scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'docker-trivy-results.sarif'

      - name: Upload Docker test results
        uses: actions/upload-artifact@v4
        with:
          name: docker-test-results
          path: |
            docker-test-report.json
            docker-trivy-results.sarif
          retention-days: 30

  # Installation Script Testing
  install-test:
    name: Installation Script Test
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Test install script syntax
        run: |
          chmod +x install.sh
          bash -n install.sh
          echo "‚úÖ Install script syntax check passed"

      - name: Test install script help
        run: |
          ./install.sh --help
          ./install.sh --version
          echo "‚úÖ Install script help test passed"

      - name: Test installation process
        run: |
          echo "üß™ Testing installation process..."
          # Create a test environment
          mkdir -p test-install
          cd test-install
          
          # Test git clone simulation
          git clone .. VigileGuard-test
          cd VigileGuard-test
          
          # Test dependency installation
          pip install -r requirements.txt
          
          # Test basic functionality after install
          python vigileguard.py --help
          echo "‚úÖ Installation process test passed"

  # Security Testing (Enhanced)
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: python

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

  # Cross-platform compatibility test
  compatibility:
    name: Cross-Platform Test
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, ubuntu-20.04, ubuntu-22.04]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test platform compatibility
        run: |
          echo "üß™ Testing on ${{ matrix.os }}..."
          python vigileguard.py --format json --output platform-test.json || true
          
          if [ -f platform-test.json ]; then
            echo "‚úÖ Platform compatibility test passed on ${{ matrix.os }}"
          else
            echo "‚ùå Platform compatibility test failed on ${{ matrix.os }}"
            exit 1
          fi

  # VigileGuard Demo (Enhanced)
  demo:
    name: VigileGuard Security Demo
    runs-on: ubuntu-latest
    needs: [test]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run VigileGuard Demo
        run: |
          echo "üõ°Ô∏è VigileGuard Security Audit Demo"
          echo "=================================="
          echo ""
          echo "Running security audit on GitHub Actions runner..."
          echo ""
          
          python vigileguard.py --format console || true
          
          echo ""
          echo "üìä Generating detailed JSON report..."
          python vigileguard.py --format json --output demo-report.json || true
          
          if [ -f demo-report.json ]; then
            echo ""
            echo "üìã Security Audit Summary:"
            python -c "
          import json
          with open('demo-report.json', 'r') as f:
              data = json.load(f)
              print(f'üîç Total findings: {data[\"summary\"][\"total_findings\"]}')
              for severity, count in data['summary']['by_severity'].items():
                  if count > 0:
                      emoji = 'üî¥' if severity in ['CRITICAL', 'HIGH'] else 'üü°' if severity == 'MEDIUM' else 'üîµ'
                      print(f'{emoji} {severity}: {count}')
              print('')
              print('‚úÖ VigileGuard successfully identified security issues!')
              print('üìñ This demonstrates the tool is working correctly.')
          "
          fi

      - name: Generate demo badge
        run: |
          echo "üè∑Ô∏è Generating demo badge..."
          python -c "
          import json
          with open('demo-report.json', 'r') as f:
              data = json.load(f)
              total = data['summary']['total_findings']
              critical_high = data['summary']['by_severity'].get('CRITICAL', 0) + data['summary']['by_severity'].get('HIGH', 0)
              
              # Create a simple badge data
              badge_data = {
                  'schemaVersion': 1,
                  'label': 'VigileGuard Demo',
                  'message': f'{total} findings ({critical_high} critical/high)',
                  'color': 'red' if critical_high > 0 else 'yellow' if total > 0 else 'green'
              }
              
              with open('demo-badge.json', 'w') as badge_file:
                  json.dump(badge_data, badge_file, indent=2)
          "

      - name: Upload demo results
        uses: actions/upload-artifact@v4
        with:
          name: vigileguard-demo
          path: |
            demo-report.json
            demo-badge.json
          retention-days: 30

  # Documentation and Examples Testing
  docs:
    name: Documentation Test
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Verify README examples
        run: |
          echo "üìö Testing README examples..."
          
          # Test all example commands in README work
          python vigileguard.py --help > /dev/null
          python vigileguard.py --version > /dev/null
          python vigileguard.py --format json --output readme-test.json || true
          
          if [ -f readme-test.json ]; then
            echo "‚úÖ README examples work correctly"
          else
            echo "‚ùå README examples failed"
            exit 1
          fi

      - name: Check documentation completeness
        run: |
          echo "üìã Checking documentation completeness..."
          
          # Verify all essential files exist
          test -f README.md && echo "‚úÖ README.md exists"
          test -f requirements.txt && echo "‚úÖ requirements.txt exists"
          test -f config.yaml && echo "‚úÖ config.yaml exists"
          test -f install.sh && echo "‚úÖ install.sh exists"
          test -f Dockerfile && echo "‚úÖ Dockerfile exists"
          test -f Makefile && echo "‚úÖ Makefile exists"
          
          # Check README has essential sections
          grep -q "Installation" README.md && echo "‚úÖ Installation section exists"
          grep -q "Usage" README.md && echo "‚úÖ Usage section exists"
          grep -q "CI/CD" README.md && echo "‚úÖ CI/CD section exists"
          grep -q "Contributing" README.md && echo "‚úÖ Contributing section exists"

  # Release preparation
  release-check:
    name: Release Readiness
    runs-on: ubuntu-latest
    if: github.event_name == 'release'
    needs: [quality, test, docker, install-test, security, compatibility]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify release readiness
        run: |
          echo "üöÄ Checking release readiness..."
          
          # Check version consistency
          VERSION=$(python vigileguard.py --version 2>/dev/null | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+' || echo "1.0.0")
          echo "Current version: $VERSION"
          
          # Verify all tests passed
          echo "‚úÖ All tests passed"
          echo "‚úÖ Security scans completed"
          echo "‚úÖ Cross-platform compatibility verified"
          echo "‚úÖ Release is ready for deployment"

      - name: Create release artifacts
        run: |
          echo "üì¶ Creating release artifacts..."
          
          # Create a release archive
          tar -czf vigileguard-${GITHUB_REF_NAME}.tar.gz \
            vigileguard.py requirements.txt config.yaml \
            install.sh Dockerfile Makefile README.md
          
          echo "‚úÖ Release artifacts created"

      - name: Upload release artifacts
        uses: actions/upload-artifact@v4
        with:
          name: release-artifacts-${{ github.ref_name }}
          path: vigileguard-${{ github.ref_name }}.tar.gz
          retention-days: 90

  # Notification and status reporting
  notify:
    name: Notification
    runs-on: ubuntu-latest
    needs: [quality, test, docker, install-test]
    if: always()
    
    steps:
      - name: Determine overall status
        id: status
        run: |
          if [[ "${{ needs.quality.result }}" == "success" && "${{ needs.test.result }}" == "success" && "${{ needs.docker.result }}" == "success" && "${{ needs.install-test.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=All VigileGuard tests passed! üéâ" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Some VigileGuard tests failed. Check the logs." >> $GITHUB_OUTPUT
          fi

      - name: Create status comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const status = '${{ steps.status.outputs.status }}';
            const message = '${{ steps.status.outputs.message }}';
            const emoji = status === 'success' ? '‚úÖ' : '‚ùå';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${emoji} **VigileGuard CI/CD Pipeline**\n\n${message}\n\n**Test Results:**\n- Code Quality: ${{ needs.quality.result }}\n- Functionality Tests: ${{ needs.test.result }}\n- Docker Build: ${{ needs.docker.result }}\n- Installation: ${{ needs.install-test.result }}`
            });

      - name: Final status check
        run: |
          echo "üõ°Ô∏è VigileGuard CI/CD Pipeline Complete"
          echo "======================================"
          echo "Status: ${{ steps.status.outputs.status }}"
          echo "Message: ${{ steps.status.outputs.message }}"
          
          if [[ "${{ steps.status.outputs.status }}" == "failure" ]]; then
            exit 1
          fi