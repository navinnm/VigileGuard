name: VigileGuard CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ published ]
  schedule:
    # Run security scan daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.8'
  PYTHON_VERSION_MATRIX: '["3.8", "3.9", "3.10", "3.11"]'

jobs:
  # Code Quality Checks
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install black flake8 mypy bandit safety

      - name: Code formatting check
        run: |
          black --check --line-length=100 vigileguard.py
          echo "✅ Code formatting check passed"

      - name: Lint with flake8
        run: |
          flake8 vigileguard.py --max-line-length=100 --ignore=E203,W503 --statistics
          echo "✅ Linting check passed"

      - name: Type checking with mypy
        run: |
          mypy vigileguard.py --ignore-missing-imports
          echo "✅ Type checking passed"
        continue-on-error: true

      - name: Security scan with bandit
        run: |
          bandit -r . -f json -o bandit-report.json
          echo "✅ Security scan completed"
        continue-on-error: true

      - name: Dependency security check
        run: |
          safety check --json --output safety-report.json
          echo "✅ Dependency security check completed"
        continue-on-error: true

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  # Multi-Python Version Testing
  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, ubuntu-20.04]
        python-version: ["3.8", "3.9", "3.10", "3.11"]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Test VigileGuard basic functionality
        run: |
          python vigileguard.py --help
          python vigileguard.py --version
          echo "✅ Basic functionality tests passed"

      - name: Test JSON output
        run: |
          python vigileguard.py --format json --output test-report.json
          test -f test-report.json
          echo "✅ JSON output test passed"

      - name: Test with custom config
        run: |
          python vigileguard.py --config config.yaml --format console
          echo "✅ Custom config test passed"

      - name: Run unit tests (if available)
        run: |
          if [ -f "test_vigileguard.py" ]; then
            pytest test_vigileguard.py -v --cov=. --cov-report=xml
          else
            echo "⚠️ No unit tests found, skipping pytest"
          fi
        continue-on-error: true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test-report.json
            coverage.xml
          retention-days: 30

  # Docker Build and Test
  docker:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: [quality]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          docker build -t vigileguard:test .
          echo "✅ Docker image built successfully"

      - name: Test Docker image
        run: |
          docker run --rm vigileguard:test --help
          docker run --rm vigileguard:test --version
          docker run --rm vigileguard:test --format json > docker-test-report.json
          echo "✅ Docker image tests passed"

      - name: Scan Docker image for vulnerabilities
        run: |
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            -v $PWD:/root/.cache/ \
            aquasec/trivy:latest image vigileguard:test
        continue-on-error: true

      - name: Upload Docker test results
        uses: actions/upload-artifact@v4
        with:
          name: docker-test-results
          path: docker-test-report.json
          retention-days: 30

  # Installation Script Testing
  install-test:
    name: Installation Script Test
    runs-on: ubuntu-latest
    needs: [test]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Test install script help
        run: |
          chmod +x install.sh
          ./install.sh --help
          ./install.sh --version

      - name: Test install script (dry run simulation)
        run: |
          # Test the script without actually installing
          bash -n install.sh
          echo "✅ Install script syntax check passed"

      - name: Test uninstall functionality
        run: |
          ./install.sh --help | grep -q "uninstall"
          echo "✅ Uninstall functionality present"

  # Performance Testing
  performance:
    name: Performance Test
    runs-on: ubuntu-latest
    needs: [test]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Performance benchmark
        run: |
          echo "🚀 Running performance tests..."
          
          # Test execution time
          time python vigileguard.py --format json > /dev/null
          
          # Test memory usage (if available)
          if command -v /usr/bin/time >/dev/null; then
            /usr/bin/time -v python vigileguard.py --format json > /dev/null
          fi
          
          echo "✅ Performance tests completed"

  # Security Audit
  security:
    name: Security Audit
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          languages: python
        continue-on-error: true

  # Documentation and Examples
  docs:
    name: Documentation Test
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify README examples
        run: |
          # Check that all example commands in README are valid
          grep -E "^  python vigileguard\.py" README.md | while read cmd; do
            echo "Testing: $cmd"
            eval "$cmd --help" > /dev/null
          done
          echo "✅ README examples verified"

      - name: Check documentation completeness
        run: |
          # Verify essential files exist
          test -f README.md && echo "✅ README.md exists"
          test -f requirements.txt && echo "✅ requirements.txt exists"
          test -f config.yaml && echo "✅ config.yaml exists"
          test -f install.sh && echo "✅ install.sh exists"
          test -f Dockerfile && echo "✅ Dockerfile exists"
          test -f Makefile && echo "✅ Makefile exists"
          
          # Check README has essential sections
          grep -q "## Installation" README.md && echo "✅ Installation section exists"
          grep -q "## Usage" README.md && echo "✅ Usage section exists"
          grep -q "## CI/CD" README.md && echo "✅ CI/CD section exists"

  # Release Preparation
  release:
    name: Release Preparation
    runs-on: ubuntu-latest
    if: github.event_name == 'release'
    needs: [quality, test, docker, install-test, performance]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Build release artifacts
        run: |
          # Create release archive
          tar -czf vigileguard-${{ github.ref_name }}.tar.gz \
            vigileguard.py requirements.txt config.yaml \
            install.sh Dockerfile Makefile README.md
          
          echo "✅ Release artifacts created"

      - name: Create Docker release image
        run: |
          docker build -t vigileguard:${{ github.ref_name }} .
          docker tag vigileguard:${{ github.ref_name }} vigileguard:latest
          echo "✅ Release Docker images created"

      - name: Upload release artifacts
        uses: actions/upload-artifact@v4
        with:
          name: release-artifacts
          path: |
            vigileguard-${{ github.ref_name }}.tar.gz
          retention-days: 90

  # Notification
  notify:
    name: Notification
    runs-on: ubuntu-latest
    needs: [quality, test, docker, install-test]
    if: always()
    
    steps:
      - name: Notify on success
        if: ${{ needs.quality.result == 'success' && needs.test.result == 'success' }}
        run: |
          echo "✅ VigileGuard CI/CD pipeline completed successfully!"
          echo "🛡️ All security checks passed"
          echo "🧪 All tests passed across Python versions"
          echo "🐳 Docker build successful"
          echo "📦 Installation script validated"

      - name: Notify on failure
        if: ${{ needs.quality.result == 'failure' || needs.test.result == 'failure' }}
        run: |
          echo "❌ VigileGuard CI/CD pipeline failed!"
          echo "Please check the failed jobs and fix issues"
          exit 1

# Workflow Status Badge
# Add this to your README.md:
# [![CI/CD](https://github.com/navinnm/VigileGuard/workflows/VigileGuard%20CI/CD%20Pipeline/badge.svg)](https://github.com/navinnm/VigileGuard/actions)